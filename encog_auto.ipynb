{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import shapiro\n",
        "import pandas as pd\n",
        "\n",
        "CONFIG_MAX_DUMMY = \"max_dummy\"\n",
        "CONFIG_MAX_DUMMY_PCT = \"pct_dummy\"\n",
        "\n",
        "CONFIG = {\n",
        "  CONFIG_MAX_DUMMY: 1000,\n",
        "  CONFIG_MAX_DUMMY_PCT: 0.75\n",
        "}\n",
        "\n",
        "def isnumeric(datatype):\n",
        "  return datatype in [FIELD_TYPE_FLOAT,FIELD_TYPE_INT]\n",
        "\n",
        "FIELDS = \"fields\"\n",
        "FIELD_ACTION = \"action\"\n",
        "FIELD_ACTION_COPY = \"copy\"\n",
        "FIELD_ACTION_IGNORE = \"ignore\"\n",
        "FIELD_ACTION_ZSCORE = \"zscore\"\n",
        "FIELD_ACTION_NORMALIZE = \"normalize\"\n",
        "FIELD_ACTION_DUMMY = \"dummy\"\n",
        "FIELD_ACTION_TARGET = \"target\"\n",
        "FIELD_NAME = \"name\"\n",
        "FIELD_SUM = \"sum\"\n",
        "FIELD_TYPE = \"type\"\n",
        "FIELD_MEAN = \"mean\"\n",
        "FIELD_NUM = \"n\"\n",
        "FIELD_MISSING = \"missing\"\n",
        "FIELD_MIN = \"min\"\n",
        "FIELD_MAX = \"max\"\n",
        "FIELD_VAR = \"var\"\n",
        "FIELD_SD = \"sd\"\n",
        "FIELD_UNIQUE = \"unique\"\n",
        "FIELD_MEDIAN = \"median\"\n",
        "FIELD_MODE = \"mode\"\n",
        "FIELD_SHAPIRO_STAT = \"shapiro-stat\"\n",
        "FIELD_SHAPIRO_P = \"shapiro-p\"\n",
        "META_TARGET = \"target\"\n",
        "META_TYPE = \"type\"\n",
        "META_TYPE_BINARY_CLASSIFICATION = \"binary-classification\"\n",
        "META_TYPE_CLASSIFICATION = \"classification\"\n",
        "META_TYPE_REGRESSION = \"regression\"\n",
        "META_SOURCE = \"source\"\n",
        "META_POSITIVE_TOKEN = \"positive-token\"\n",
        "\n",
        "FIELD_TYPE_FLOAT = \"float\"\n",
        "FIELD_TYPE_INT = \"int\"\n",
        "FIELD_TYPE_STR = \"str\"\n",
        "\n",
        "def find_positive(s):\n",
        "  s = set(s.str.upper().tolist())\n",
        "  if len(s) != 2: return None\n",
        "  if \"+\" in s and \"-\" in s: return \"+\"\n",
        "  if \"0\" in s and \"1\" in s: return \"1\"\n",
        "  if \"t\" in s and \"f\" in s: return \"t\"\n",
        "  if \"y\" in s and \"n\" in s: return \"y\"\n",
        "  if \"true\" in s and \"false\" in s: return \"true\"\n",
        "  if \"yes\" in s and \"no\" in s: return \"no\"\n",
        "  if \"p\" in s and \"n\" in s: return \"p\"\n",
        "  if \"positive\" in s and \"negative\" in s: return \"positive\"\n",
        "  s = list(s)\n",
        "  s.sort()\n",
        "  return s[0]\n",
        "\n",
        "def analyze(data_source, target, is_regression=True):\n",
        "  df = pd.read_csv(data_source,na_values=['NA', '?'])\n",
        "\n",
        "  metadata = {\n",
        "      FIELDS: {},\n",
        "      META_TARGET: target,\n",
        "      META_SOURCE: data_source,\n",
        "  }\n",
        "\n",
        "  fields = metadata[FIELDS]\n",
        "\n",
        "  for field_name,csv_type in zip(df.columns,df.dtypes):\n",
        "    #print(name,csv_type)\n",
        "    if \"float\" in csv_type.name:\n",
        "      dtype = FIELD_TYPE_FLOAT\n",
        "      action = FIELD_ACTION_COPY\n",
        "    elif \"int\" in csv_type.name:\n",
        "      dtype = FIELD_TYPE_INT\n",
        "      action = FIELD_ACTION_COPY\n",
        "    else:\n",
        "      dtype = FIELD_TYPE_STR\n",
        "      action = FIELD_ACTION_IGNORE\n",
        "\n",
        "    missing_count = sum(df[field_name].isnull())\n",
        "    col = df[field_name]\n",
        "    unique_count = len(pd.unique(col))\n",
        "\n",
        "    if isnumeric(dtype):\n",
        "      stat, p = shapiro(col)\n",
        "\n",
        "      # less than or equal to 0.05 not normal\n",
        "      action = FIELD_ACTION_ZSCORE if p>0.05 else FIELD_ACTION_NORMALIZE\n",
        "\n",
        "      fields[field_name] = {\n",
        "          FIELD_TYPE:dtype,\n",
        "          FIELD_MEDIAN:col.median(),\n",
        "          FIELD_MEAN:col.mean(),\n",
        "          FIELD_SD:col.std(),\n",
        "          FIELD_MAX:col.max(),\n",
        "          FIELD_MIN:col.min(),\n",
        "          FIELD_SHAPIRO_STAT:stat,\n",
        "          FIELD_SHAPIRO_P:p,\n",
        "          FIELD_ACTION:action,\n",
        "          FIELD_MISSING:missing_count,\n",
        "          FIELD_UNIQUE:unique_count}\n",
        "\n",
        "    else:\n",
        "      fields[field_name] = {\n",
        "          FIELD_TYPE:dtype,\n",
        "          FIELD_MODE:col.mode()[0],\n",
        "          FIELD_ACTION:action,\n",
        "          FIELD_MISSING:missing_count,\n",
        "          FIELD_UNIQUE:unique_count}\n",
        "\n",
        "    # Determine action\n",
        "    field = fields[field_name]\n",
        "    if (field[FIELD_TYPE] == FIELD_TYPE_STR) and (field[FIELD_UNIQUE]<CONFIG[CONFIG_MAX_DUMMY]) and (field[FIELD_UNIQUE]/len(df)<CONFIG[CONFIG_MAX_DUMMY_PCT]):\n",
        "      field[FIELD_ACTION] = FIELD_ACTION_DUMMY\n",
        "    if field_name == target:\n",
        "      field[FIELD_ACTION] = FIELD_ACTION_TARGET\n",
        "  \n",
        "  # Determine model type\n",
        "  is_binary = (metadata[FIELDS][target][FIELD_UNIQUE]==2) and not is_regression\n",
        "\n",
        "  if is_regression:\n",
        "    metadata[META_TYPE] = META_TYPE_REGRESSION\n",
        "  else:\n",
        "    if metadata[FIELDS][target][FIELD_UNIQUE]==2:\n",
        "      metadata[META_TYPE] = META_TYPE_BINARY_CLASSIFICATION\n",
        "\n",
        "      metadata[META_POSITIVE_TOKEN] = find_positive(df[target])\n",
        "    else:\n",
        "      metadata[META_TYPE] = META_TYPE_CLASSIFICATION\n",
        "\n",
        "  return metadata\n",
        "\n",
        "COLS = [FIELD_MEAN, FIELD_SD, FIELD_MEDIAN, FIELD_MODE, FIELD_MAX, FIELD_ACTION, FIELD_UNIQUE, FIELD_SHAPIRO_P,FIELD_MISSING]\n",
        "\n",
        "def field_summary(metadata, cols=COLS):\n",
        "  data = {}\n",
        "\n",
        "  data['name'] = []\n",
        "  for col in cols:\n",
        "    data[col] = []\n",
        "\n",
        "  for field_name in metadata[FIELDS]:\n",
        "    field = metadata[FIELDS][field_name]\n",
        "    data['name'].append(field_name)\n",
        "    for col in cols:\n",
        "      data[col].append(field.get(col, None))\n",
        "\n",
        "  return pd.DataFrame(data)[['name']+COLS]\n",
        "\n",
        "#url = \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\"; target = \"mpg\";is_regression=True\n",
        "#url = \"https://data.heatonresearch.com/data/t81-558/iris.csv\"; target = \"species\";is_regression=False\n",
        "url = \"https://data.heatonresearch.com/data/t81-558/crx.csv\"; target = 'a16'; is_regression=False\n",
        "\n",
        "metadata = analyze(url, target, is_regression)\n",
        "print(metadata)\n",
        "  "
      ],
      "metadata": {
        "id": "DSCFz5EBU4DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import MISSING\n",
        "from pandas.core.dtypes.inference import is_re\n",
        "def tolist(obj):\n",
        "  if isinstance(obj,list) or isinstance(obj, tuple):\n",
        "    return obj\n",
        "  else:\n",
        "    return [obj]\n",
        "\n",
        "class PythonFile:\n",
        "  def __init__(self):\n",
        "    self.imports = []\n",
        "    self.lines = []\n",
        "\n",
        "  def add_import(self, name, alias=None):\n",
        "    if alias:\n",
        "      self.imports.append({\"name\": name, \"alias\": alias})\n",
        "    else:\n",
        "      self.imports.append({\"name\": name})\n",
        "\n",
        "  def add_from(self, _from, _import):\n",
        "    self.imports.append({\"from\": _from, \"import\": _import})\n",
        "    \n",
        "  def generate(self):\n",
        "    src = \"\"\n",
        "    for obj in self.imports:\n",
        "      if \"name\" in obj and \"alias\" in obj:\n",
        "        src += f\"import {obj['name']} as {obj['alias']}\"\n",
        "      elif \"name\" in obj and \"alias\" not in obj:\n",
        "        src += f\"import {obj['name']}\"\n",
        "      elif \"from\" in obj and \"import\" in obj:\n",
        "        imports = \", \".join(tolist(obj['import']))\n",
        "        src += f\"from {obj['from']} import {imports}\"\n",
        "\n",
        "      src+=\"\\n\"\n",
        "\n",
        "    for line in self.lines:\n",
        "      src+=line+\"\\n\"\n",
        "    return src\n",
        "\n",
        "  def add_line(self, str):\n",
        "    self.lines.append(str)\n",
        "\n",
        "  def call(self, name, *args):\n",
        "    src = name + \"(\"\n",
        "\n",
        "    formatted_args = []\n",
        "    started_named = False\n",
        "    for arg in args:\n",
        "      if isinstance(arg,dict):\n",
        "        formatted_args += [f\"{name}={arg[name]}\" for name in arg.keys()]\n",
        "        started_named = True\n",
        "      else: \n",
        "        if started_named: raise ValueError(\"positional argument follows keyword argument\")\n",
        "        formatted_args.append(str(arg))\n",
        "\n",
        "    src += \", \".join(formatted_args)\n",
        "    src += \")\"\n",
        "    return src\n",
        "\n",
        "  def assign(self, left, right):\n",
        "    return f\"{left} = {right}\"\n",
        "\n",
        "  def str(self, str):\n",
        "    return f\"\\\"{str}\\\"\"\n",
        "\n",
        "  def index(self, name, indexes, dot=None):\n",
        "    src = name\n",
        "    for idx in indexes:\n",
        "      src+=f'[{idx}]'\n",
        "\n",
        "    if dot:\n",
        "      src+='.'\n",
        "      src+=dot\n",
        "    return src\n",
        "    \n",
        "def generate_keras(metadata):\n",
        "  na_values = ['NA', '?']\n",
        "  target = metadata[META_TARGET]\n",
        "  is_regression = metadata[META_TYPE] == META_TYPE_REGRESSION\n",
        "  is_binary = (metadata[FIELDS][target][FIELD_UNIQUE]==2) and (metadata[META_TYPE]==META_TYPE_CLASSIFICATION)\n",
        "\n",
        "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
        "    loss = \"mean_squared_error\"\n",
        "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
        "    loss = \"binary_crossentropy\"\n",
        "  else:\n",
        "    loss = \"categorical_crossentropy\"\n",
        "\n",
        "  py = PythonFile()\n",
        "  py.add_import(\"pandas\", \"pd\")\n",
        "  py.add_import(\"io\")\n",
        "  py.add_import(\"requests\")\n",
        "  py.add_import(\"numpy\", \"np\")\n",
        "  py.add_from(\"tensorflow.keras.models\", \"Sequential\")\n",
        "  py.add_from(\"tensorflow.keras.layers\", [\"Dense\", \"Activation\"])\n",
        "  py.add_from(\"tensorflow.keras.callbacks\", \"EarlyStopping\")\n",
        "  py.add_from(\"scipy.stats\", \"zscore\")\n",
        "  py.add_from(\"sklearn.preprocessing\", \"MinMaxScaler\")\n",
        "\n",
        "  x_fields = [x for x in metadata[FIELDS] if x != target and metadata[FIELDS][x][FIELD_ACTION] in [FIELD_ACTION_COPY]]\n",
        "  py.add_line(py.assign(\"x_fields\",x_fields))\n",
        "  py.add_line(py.assign(\"df\", py.call(\"pd.read_csv\",py.str(metadata[META_SOURCE]),{'na_values':na_values})))\n",
        "\n",
        "  for field_name in metadata[FIELDS]:\n",
        "    field = metadata[FIELDS][field_name]\n",
        "    if field[FIELD_MISSING]>0:\n",
        "      if isnumeric(field[FIELD_TYPE]):\n",
        "        fn = \"median\"\n",
        "        suffix = \"\"\n",
        "      else:\n",
        "        fn = \"mode\"\n",
        "        suffix = \"[0]\"\n",
        "      py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
        "                py.index(\"df\",[py.str(field_name)],py.call(\"fillna\",\n",
        "                py.index(\"df\",[py.str(field_name)],py.call(fn)+suffix)\n",
        "                ))))\n",
        "    if field[FIELD_ACTION] == FIELD_ACTION_ZSCORE:\n",
        "      py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
        "                py.call(\"zscore\",py.index(\"df\",[py.str(field_name)]))))\n",
        "      py.add_line(py.call(\"x_fields.append\",py.str(field_name)))\n",
        "      #x_fields.append(field_name)\n",
        "    elif field[FIELD_ACTION] == FIELD_ACTION_NORMALIZE:\n",
        "      #min_val = field[FIELD_MIN]\n",
        "      #max_val = field[FIELD_MAX]\n",
        "      f1 = py.index(\"df\",[py.str(field_name)])\n",
        "      f2 = py.index(\"df\",[[field_name]])\n",
        "      #py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
        "      #                      f\"({f}-{min_val})/{max_val-min_val}\"))\n",
        "      py.add_line(py.assign(f1,py.call(\"MinMaxScaler().fit_transform\",f2)))\n",
        "      py.add_line(py.call(\"x_fields.append\",py.str(field_name)))\n",
        "      #x_fields.append(field_name)\n",
        "    elif field[FIELD_ACTION] == FIELD_ACTION_DUMMY:\n",
        "      py.add_line(py.assign(\"dummies\", \n",
        "            py.call(\"pd.get_dummies\",\n",
        "            py.index('df',[py.str(field_name)]),\n",
        "            {'prefix':py.str(field_name),'drop_first':'True'})))\n",
        "      py.add_line(\"df = pd.concat([df,dummies],axis=1)\")\n",
        "      #py.add_line(py.call(\"df.drop\",py.str(field_name), {'axis':1, 'inplace': 'True'}))\n",
        "      py.add_line(\"x_fields += dummies.columns.tolist()\")\n",
        "      \n",
        "\n",
        "\n",
        "  py.add_line(py.assign(\"x\",py.index(\"df\",[\"x_fields\"],\"values\")))\n",
        "\n",
        "  if metadata[META_TYPE] == META_TYPE_CLASSIFICATION:\n",
        "    py.add_line(py.assign(\"dummies\", py.call(\"pd.get_dummies\", py.index(\"df\", [py.str(target)]))))\n",
        "    py.add_line(py.assign(\"species\", \"dummies.columns\"))\n",
        "    py.add_line(py.assign(\"y\", \"dummies.values\"))\n",
        "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
        "    t = py.index(\"df\",[py.str(target)])\n",
        "    pos = metadata[META_POSITIVE_TOKEN]\n",
        "    py.add_line(py.assign(t,f\"({t}=={py.str(pos)}).astype(int)\"))\n",
        "    py.add_line(py.assign(\"y\", f\"df.{target}.values\"))\n",
        "  else:\n",
        "    py.add_line(py.assign(\"y\", f\"df.{target}.values\"))\n",
        "\n",
        "  py.add_line(py.assign(\"model\",py.call(\"Sequential\")))\n",
        "  py.add_line(py.call(\"model.add\", py.call(\"Dense\",50,{\"input_dim\":\"x.shape[1]\", \"activation\":py.str('relu')})))\n",
        "  py.add_line(py.call(\"model.add\", py.call(\"Dense\",25,{\"activation\":py.str('relu')})))\n",
        "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
        "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"1\")))\n",
        "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
        "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"1\",{\"activation\":py.str('sigmoid')})))\n",
        "  else:  \n",
        "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"y.shape[1]\",{\"activation\":py.str('softmax')})))\n",
        "  py.add_line(py.call(\"model.compile\", {\"loss\":py.str(loss), \"optimizer\":py.str('adam')}))\n",
        "  py.add_line(py.call(\"model.fit\", \"x\", \"y\", {'verbose':'2','epochs':100}))\n",
        "  print(py.generate())\n",
        "\n",
        "generate_keras(metadata)\n",
        "\n",
        "#py = PythonFile()\n",
        "#py.call(\"test\",\"a\",{'b':'bval','c':'cval'})"
      ],
      "metadata": {
        "id": "U3p2kabMhX1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from scipy.stats import zscore\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "x_fields = []\n",
        "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/crx.csv\", na_values=['NA', '?'])\n",
        "df[\"a1\"] = df[\"a1\"].fillna(df[\"a1\"].mode()[0])\n",
        "dummies = pd.get_dummies(df[\"a1\"], prefix=\"a1\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a2\"] = df[\"a2\"].fillna(df[\"a2\"].median())\n",
        "df[\"a2\"] = zscore(df[\"a2\"])\n",
        "x_fields.append(\"a2\")\n",
        "df[\"s3\"] = MinMaxScaler().fit_transform(df[['s3']])\n",
        "x_fields.append(\"s3\")\n",
        "df[\"a4\"] = df[\"a4\"].fillna(df[\"a4\"].mode()[0])\n",
        "dummies = pd.get_dummies(df[\"a4\"], prefix=\"a4\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a5\"] = df[\"a5\"].fillna(df[\"a5\"].mode()[0])\n",
        "dummies = pd.get_dummies(df[\"a5\"], prefix=\"a5\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a6\"] = df[\"a6\"].fillna(df[\"a6\"].mode()[0])\n",
        "dummies = pd.get_dummies(df[\"a6\"], prefix=\"a6\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a7\"] = df[\"a7\"].fillna(df[\"a7\"].mode()[0])\n",
        "dummies = pd.get_dummies(df[\"a7\"], prefix=\"a7\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a8\"] = MinMaxScaler().fit_transform(df[['a8']])\n",
        "x_fields.append(\"a8\")\n",
        "dummies = pd.get_dummies(df[\"a9\"], prefix=\"a9\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "dummies = pd.get_dummies(df[\"a10\"], prefix=\"a10\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a11\"] = MinMaxScaler().fit_transform(df[['a11']])\n",
        "x_fields.append(\"a11\")\n",
        "dummies = pd.get_dummies(df[\"a12\"], prefix=\"a12\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "dummies = pd.get_dummies(df[\"a13\"], prefix=\"a13\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a14\"] = df[\"a14\"].fillna(df[\"a14\"].median())\n",
        "df[\"a14\"] = zscore(df[\"a14\"])\n",
        "x_fields.append(\"a14\")\n",
        "df[\"a15\"] = MinMaxScaler().fit_transform(df[['a15']])\n",
        "x_fields.append(\"a15\")\n",
        "x = df[x_fields].values\n",
        "df[\"a16\"] = (df[\"a16\"]==\"+\").astype(int)\n",
        "y = df.a16.values\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=x.shape[1], activation=\"relu\"))\n",
        "model.add(Dense(25, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "model.fit(x, y, verbose=2, epochs=100)"
      ],
      "metadata": {
        "id": "exzeWtiInF5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['a16']"
      ],
      "metadata": {
        "id": "cq35lJHtBGjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/crx.csv\", na_values=['NA', '?'])"
      ],
      "metadata": {
        "id": "OzkKfoNMOM7Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}