{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import shapiro\n",
        "import pandas as pd\n",
        "\n",
        "CONFIG_MAX_DUMMY = \"max_dummy\"\n",
        "CONFIG_MAX_DUMMY_PCT = \"pct_dummy\"\n",
        "\n",
        "CONFIG = {\n",
        "  CONFIG_MAX_DUMMY: 1000,\n",
        "  CONFIG_MAX_DUMMY_PCT: 0.75\n",
        "}\n",
        "\n",
        "def isnumeric(datatype):\n",
        "  return datatype in [FIELD_TYPE_FLOAT,FIELD_TYPE_INT]\n",
        "\n",
        "FIELDS = \"fields\"\n",
        "FIELD_ACTION = \"action\"\n",
        "FIELD_ACTION_COPY = \"copy\"\n",
        "FIELD_ACTION_IGNORE = \"ignore\"\n",
        "FIELD_ACTION_ZSCORE = \"zscore\"\n",
        "FIELD_ACTION_NORMALIZE = \"normalize\"\n",
        "FIELD_ACTION_DUMMY = \"dummy\"\n",
        "FIELD_ACTION_TARGET = \"target\"\n",
        "FIELD_NAME = \"name\"\n",
        "FIELD_SUM = \"sum\"\n",
        "FIELD_TYPE = \"type\"\n",
        "FIELD_MEAN = \"mean\"\n",
        "FIELD_NUM = \"n\"\n",
        "FIELD_MISSING = \"missing\"\n",
        "FIELD_MIN = \"min\"\n",
        "FIELD_MAX = \"max\"\n",
        "FIELD_VAR = \"var\"\n",
        "FIELD_SD = \"sd\"\n",
        "FIELD_UNIQUE = \"unique\"\n",
        "FIELD_MEDIAN = \"median\"\n",
        "FIELD_MODE = \"mode\"\n",
        "FIELD_SHAPIRO_STAT = \"shapiro-stat\"\n",
        "FIELD_SHAPIRO_P = \"shapiro-p\"\n",
        "META_TARGET = \"target\"\n",
        "META_TYPE = \"type\"\n",
        "META_TYPE_BINARY_CLASSIFICATION = \"binary-classification\"\n",
        "META_TYPE_CLASSIFICATION = \"classification\"\n",
        "META_TYPE_REGRESSION = \"regression\"\n",
        "META_SOURCE = \"source\"\n",
        "META_POSITIVE_TOKEN = \"positive-token\"\n",
        "\n",
        "FIELD_TYPE_FLOAT = \"float\"\n",
        "FIELD_TYPE_INT = \"int\"\n",
        "FIELD_TYPE_STR = \"str\"\n",
        "\n",
        "def find_positive(s):\n",
        "  s = set(s.str.upper().tolist())\n",
        "  if len(s) != 2: return None\n",
        "  if \"+\" in s and \"-\" in s: return \"+\"\n",
        "  if \"0\" in s and \"1\" in s: return \"1\"\n",
        "  if \"t\" in s and \"f\" in s: return \"t\"\n",
        "  if \"y\" in s and \"n\" in s: return \"y\"\n",
        "  if \"true\" in s and \"false\" in s: return \"true\"\n",
        "  if \"yes\" in s and \"no\" in s: return \"no\"\n",
        "  if \"p\" in s and \"n\" in s: return \"p\"\n",
        "  if \"positive\" in s and \"negative\" in s: return \"positive\"\n",
        "  s = list(s)\n",
        "  s.sort()\n",
        "  return s[0]\n",
        "\n",
        "def analyze(data_source, target, is_regression=True):\n",
        "  df = pd.read_csv(data_source,na_values=['NA', '?'])\n",
        "\n",
        "  metadata = {\n",
        "      FIELDS: {},\n",
        "      META_TARGET: target,\n",
        "      META_SOURCE: data_source,\n",
        "  }\n",
        "\n",
        "  fields = metadata[FIELDS]\n",
        "\n",
        "  for field_name,csv_type in zip(df.columns,df.dtypes):\n",
        "    #print(name,csv_type)\n",
        "    if \"float\" in csv_type.name:\n",
        "      dtype = FIELD_TYPE_FLOAT\n",
        "      action = FIELD_ACTION_COPY\n",
        "    elif \"int\" in csv_type.name:\n",
        "      dtype = FIELD_TYPE_INT\n",
        "      action = FIELD_ACTION_COPY\n",
        "    else:\n",
        "      dtype = FIELD_TYPE_STR\n",
        "      action = FIELD_ACTION_IGNORE\n",
        "\n",
        "    missing_count = sum(df[field_name].isnull())\n",
        "    col = df[field_name]\n",
        "    unique_count = len(pd.unique(col))\n",
        "\n",
        "    if isnumeric(dtype):\n",
        "      stat, p = shapiro(col)\n",
        "\n",
        "      # less than or equal to 0.05 not normal\n",
        "      action = FIELD_ACTION_ZSCORE if p>0.05 else FIELD_ACTION_NORMALIZE\n",
        "\n",
        "      fields[field_name] = {\n",
        "          FIELD_TYPE:dtype,\n",
        "          FIELD_MEDIAN:col.median(),\n",
        "          FIELD_MEAN:col.mean(),\n",
        "          FIELD_SD:col.std(),\n",
        "          FIELD_MAX:col.max(),\n",
        "          FIELD_MIN:col.min(),\n",
        "          FIELD_SHAPIRO_STAT:stat,\n",
        "          FIELD_SHAPIRO_P:p,\n",
        "          FIELD_ACTION:action,\n",
        "          FIELD_MISSING:missing_count,\n",
        "          FIELD_UNIQUE:unique_count}\n",
        "\n",
        "    else:\n",
        "      fields[field_name] = {\n",
        "          FIELD_TYPE:dtype,\n",
        "          FIELD_MODE:col.mode()[0],\n",
        "          FIELD_ACTION:action,\n",
        "          FIELD_MISSING:missing_count,\n",
        "          FIELD_UNIQUE:unique_count}\n",
        "\n",
        "    # Determine action\n",
        "    field = fields[field_name]\n",
        "    if (field[FIELD_TYPE] == FIELD_TYPE_STR) and (field[FIELD_UNIQUE]<CONFIG[CONFIG_MAX_DUMMY]) and (field[FIELD_UNIQUE]/len(df)<CONFIG[CONFIG_MAX_DUMMY_PCT]):\n",
        "      field[FIELD_ACTION] = FIELD_ACTION_DUMMY\n",
        "    if field_name == target:\n",
        "      field[FIELD_ACTION] = FIELD_ACTION_TARGET\n",
        "  \n",
        "  # Determine model type\n",
        "  is_binary = (metadata[FIELDS][target][FIELD_UNIQUE]==2) and not is_regression\n",
        "\n",
        "  if is_regression:\n",
        "    metadata[META_TYPE] = META_TYPE_REGRESSION\n",
        "  else:\n",
        "    if metadata[FIELDS][target][FIELD_UNIQUE]==2:\n",
        "      metadata[META_TYPE] = META_TYPE_BINARY_CLASSIFICATION\n",
        "\n",
        "      metadata[META_POSITIVE_TOKEN] = find_positive(df[target])\n",
        "    else:\n",
        "      metadata[META_TYPE] = META_TYPE_CLASSIFICATION\n",
        "\n",
        "  return metadata\n",
        "\n",
        "COLS = [FIELD_MEAN, FIELD_SD, FIELD_MEDIAN, FIELD_MODE, FIELD_MAX, FIELD_ACTION, FIELD_UNIQUE, FIELD_SHAPIRO_P,FIELD_MISSING]\n",
        "\n",
        "def field_summary(metadata, cols=COLS):\n",
        "  data = {}\n",
        "\n",
        "  data['name'] = []\n",
        "  for col in cols:\n",
        "    data[col] = []\n",
        "\n",
        "  for field_name in metadata[FIELDS]:\n",
        "    field = metadata[FIELDS][field_name]\n",
        "    data['name'].append(field_name)\n",
        "    for col in cols:\n",
        "      data[col].append(field.get(col, None))\n",
        "\n",
        "  return pd.DataFrame(data)[['name']+COLS]\n",
        "\n",
        "#url = \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\"; target = \"mpg\";is_regression=True\n",
        "#url = \"https://data.heatonresearch.com/data/t81-558/iris.csv\"; target = \"species\";is_regression=False\n",
        "url = \"https://data.heatonresearch.com/data/t81-558/crx.csv\"; target = 'a16'; is_regression=False\n",
        "\n",
        "metadata = analyze(url, target, is_regression)\n",
        "print(metadata)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSCFz5EBU4DN",
        "outputId": "f86ecafe-0080-4ecf-9b00-15757e57c98f"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fields': {'a1': {'type': 'str', 'mode': 'b', 'action': 'dummy', 'missing': 12, 'unique': 3}, 'a2': {'type': 'float', 'median': 28.46, 'mean': 31.56817109144543, 'sd': 11.957862498270877, 'max': 80.25, 'min': 13.75, 'shapiro-stat': nan, 'shapiro-p': 1.0, 'action': 'zscore', 'missing': 12, 'unique': 350}, 's3': {'type': 'float', 'median': 2.75, 'mean': 4.758724637681159, 'sd': 4.978163248528541, 'max': 28.0, 'min': 0.0, 'shapiro-stat': 0.8302544355392456, 'shapiro-p': 2.0092734071726269e-26, 'action': 'normalize', 'missing': 0, 'unique': 215}, 'a4': {'type': 'str', 'mode': 'u', 'action': 'dummy', 'missing': 6, 'unique': 4}, 'a5': {'type': 'str', 'mode': 'g', 'action': 'dummy', 'missing': 6, 'unique': 4}, 'a6': {'type': 'str', 'mode': 'c', 'action': 'dummy', 'missing': 9, 'unique': 15}, 'a7': {'type': 'str', 'mode': 'v', 'action': 'dummy', 'missing': 9, 'unique': 10}, 'a8': {'type': 'float', 'median': 1.0, 'mean': 2.223405797101449, 'sd': 3.3465133592781324, 'max': 28.5, 'min': 0.0, 'shapiro-stat': 0.665294885635376, 'shapiro-p': 1.109698924394265e-34, 'action': 'normalize', 'missing': 0, 'unique': 132}, 'a9': {'type': 'str', 'mode': 't', 'action': 'dummy', 'missing': 0, 'unique': 2}, 'a10': {'type': 'str', 'mode': 'f', 'action': 'dummy', 'missing': 0, 'unique': 2}, 'a11': {'type': 'int', 'median': 0.0, 'mean': 2.4, 'sd': 4.862940034226996, 'max': 67, 'min': 0, 'shapiro-stat': 0.5330631732940674, 'shapiro-p': 3.662308950796017e-39, 'action': 'normalize', 'missing': 0, 'unique': 23}, 'a12': {'type': 'str', 'mode': 'f', 'action': 'dummy', 'missing': 0, 'unique': 2}, 'a13': {'type': 'str', 'mode': 'g', 'action': 'dummy', 'missing': 0, 'unique': 3}, 'a14': {'type': 'float', 'median': 160.0, 'mean': 184.01477104874445, 'sd': 173.80676822523813, 'max': 2000.0, 'min': 0.0, 'shapiro-stat': nan, 'shapiro-p': 1.0, 'action': 'zscore', 'missing': 13, 'unique': 171}, 'a15': {'type': 'int', 'median': 5.0, 'mean': 1017.3855072463768, 'sd': 5210.10259830269, 'max': 100000, 'min': 0, 'shapiro-stat': 0.16985440254211426, 'shapiro-p': 0.0, 'action': 'normalize', 'missing': 0, 'unique': 240}, 'a16': {'type': 'str', 'mode': '-', 'action': 'target', 'missing': 0, 'unique': 2}}, 'target': 'a16', 'source': 'https://data.heatonresearch.com/data/t81-558/crx.csv', 'type': 'binary-classification', 'positive-token': '+'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import MISSING\n",
        "from pandas.core.dtypes.inference import is_re\n",
        "def tolist(obj):\n",
        "  if isinstance(obj,list) or isinstance(obj, tuple):\n",
        "    return obj\n",
        "  else:\n",
        "    return [obj]\n",
        "\n",
        "class PythonFile:\n",
        "  def __init__(self):\n",
        "    self.imports = []\n",
        "    self.lines = []\n",
        "\n",
        "  def add_import(self, name, alias=None):\n",
        "    if alias:\n",
        "      self.imports.append({\"name\": name, \"alias\": alias})\n",
        "    else:\n",
        "      self.imports.append({\"name\": name})\n",
        "\n",
        "  def add_from(self, _from, _import):\n",
        "    self.imports.append({\"from\": _from, \"import\": _import})\n",
        "    \n",
        "  def generate(self):\n",
        "    src = \"\"\n",
        "    for obj in self.imports:\n",
        "      if \"name\" in obj and \"alias\" in obj:\n",
        "        src += f\"import {obj['name']} as {obj['alias']}\"\n",
        "      elif \"name\" in obj and \"alias\" not in obj:\n",
        "        src += f\"import {obj['name']}\"\n",
        "      elif \"from\" in obj and \"import\" in obj:\n",
        "        imports = \", \".join(tolist(obj['import']))\n",
        "        src += f\"from {obj['from']} import {imports}\"\n",
        "\n",
        "      src+=\"\\n\"\n",
        "\n",
        "    for line in self.lines:\n",
        "      src+=line+\"\\n\"\n",
        "    return src\n",
        "\n",
        "  def add_line(self, str):\n",
        "    self.lines.append(str)\n",
        "\n",
        "  def call(self, name, *args):\n",
        "    src = name + \"(\"\n",
        "\n",
        "    formatted_args = []\n",
        "    started_named = False\n",
        "    for arg in args:\n",
        "      if isinstance(arg,dict):\n",
        "        formatted_args += [f\"{name}={arg[name]}\" for name in arg.keys()]\n",
        "        started_named = True\n",
        "      else: \n",
        "        if started_named: raise ValueError(\"positional argument follows keyword argument\")\n",
        "        formatted_args.append(str(arg))\n",
        "\n",
        "    src += \", \".join(formatted_args)\n",
        "    src += \")\"\n",
        "    return src\n",
        "\n",
        "  def assign(self, left, right):\n",
        "    return f\"{left} = {right}\"\n",
        "\n",
        "  def str(self, str):\n",
        "    return f\"\\\"{str}\\\"\"\n",
        "\n",
        "  def index(self, name, indexes, dot=None):\n",
        "    src = name\n",
        "    for idx in indexes:\n",
        "      src+=f'[{idx}]'\n",
        "\n",
        "    if dot:\n",
        "      src+='.'\n",
        "      src+=dot\n",
        "    return src\n",
        "    \n",
        "def generate_keras(metadata):\n",
        "  na_values = ['NA', '?']\n",
        "  target = metadata[META_TARGET]\n",
        "  is_regression = metadata[META_TYPE] == META_TYPE_REGRESSION\n",
        "  is_binary = (metadata[FIELDS][target][FIELD_UNIQUE]==2) and (metadata[META_TYPE]==META_TYPE_CLASSIFICATION)\n",
        "\n",
        "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
        "    loss = \"mean_squared_error\"\n",
        "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
        "    loss = \"binary_crossentropy\"\n",
        "  else:\n",
        "    loss = \"categorical_crossentropy\"\n",
        "\n",
        "  py = PythonFile()\n",
        "  py.add_import(\"pandas\", \"pd\")\n",
        "  py.add_import(\"io\")\n",
        "  py.add_import(\"requests\")\n",
        "  py.add_import(\"numpy\", \"np\")\n",
        "  py.add_from(\"tensorflow.keras.models\", \"Sequential\")\n",
        "  py.add_from(\"tensorflow.keras.layers\", [\"Dense\", \"Activation\"])\n",
        "  py.add_from(\"tensorflow.keras.callbacks\", \"EarlyStopping\")\n",
        "  py.add_from(\"scipy.stats\", \"zscore\")\n",
        "  py.add_from(\"sklearn.preprocessing\", \"MinMaxScaler\")\n",
        "\n",
        "  x_fields = [x for x in metadata[FIELDS] if x != target and metadata[FIELDS][x][FIELD_ACTION] in [FIELD_ACTION_COPY]]\n",
        "  py.add_line(py.assign(\"x_fields\",x_fields))\n",
        "  py.add_line(py.assign(\"df\", py.call(\"pd.read_csv\",py.str(metadata[META_SOURCE]),{'na_values':na_values})))\n",
        "\n",
        "  for field_name in metadata[FIELDS]:\n",
        "    field = metadata[FIELDS][field_name]\n",
        "    if field[FIELD_MISSING]>0:\n",
        "      if isnumeric(field[FIELD_TYPE]):\n",
        "        fn = \"median\"\n",
        "        suffix = \"\"\n",
        "      else:\n",
        "        fn = \"mode\"\n",
        "        suffix = \"[0]\"\n",
        "      py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
        "                py.index(\"df\",[py.str(field_name)],py.call(\"fillna\",\n",
        "                py.index(\"df\",[py.str(field_name)],py.call(fn)+suffix)\n",
        "                ))))\n",
        "    if field[FIELD_ACTION] == FIELD_ACTION_ZSCORE:\n",
        "      py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
        "                py.call(\"zscore\",py.index(\"df\",[py.str(field_name)]))))\n",
        "      py.add_line(py.call(\"x_fields.append\",py.str(field_name)))\n",
        "      #x_fields.append(field_name)\n",
        "    elif field[FIELD_ACTION] == FIELD_ACTION_NORMALIZE:\n",
        "      #min_val = field[FIELD_MIN]\n",
        "      #max_val = field[FIELD_MAX]\n",
        "      f1 = py.index(\"df\",[py.str(field_name)])\n",
        "      f2 = py.index(\"df\",[[field_name]])\n",
        "      #py.add_line(py.assign(py.index(\"df\",[py.str(field_name)]),\n",
        "      #                      f\"({f}-{min_val})/{max_val-min_val}\"))\n",
        "      py.add_line(py.assign(f1,py.call(\"MinMaxScaler().fit_transform\",f2)))\n",
        "      py.add_line(py.call(\"x_fields.append\",py.str(field_name)))\n",
        "      #x_fields.append(field_name)\n",
        "    elif field[FIELD_ACTION] == FIELD_ACTION_DUMMY:\n",
        "      py.add_line(py.assign(\"dummies\", \n",
        "            py.call(\"pd.get_dummies\",\n",
        "            py.index('df',[py.str(field_name)]),\n",
        "            {'prefix':py.str(field_name),'drop_first':'True'})))\n",
        "      py.add_line(\"df = pd.concat([df,dummies],axis=1)\")\n",
        "      #py.add_line(py.call(\"df.drop\",py.str(field_name), {'axis':1, 'inplace': 'True'}))\n",
        "      py.add_line(\"x_fields += dummies.columns.tolist()\")\n",
        "      \n",
        "\n",
        "\n",
        "  py.add_line(py.assign(\"x\",py.index(\"df\",[\"x_fields\"],\"values\")))\n",
        "\n",
        "  if metadata[META_TYPE] == META_TYPE_CLASSIFICATION:\n",
        "    py.add_line(py.assign(\"dummies\", py.call(\"pd.get_dummies\", py.index(\"df\", [py.str(target)]))))\n",
        "    py.add_line(py.assign(\"species\", \"dummies.columns\"))\n",
        "    py.add_line(py.assign(\"y\", \"dummies.values\"))\n",
        "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
        "    t = py.index(\"df\",[py.str(target)])\n",
        "    pos = metadata[META_POSITIVE_TOKEN]\n",
        "    py.add_line(py.assign(t,f\"({t}=={py.str(pos)}).astype(int)\"))\n",
        "    py.add_line(py.assign(\"y\", f\"df.{target}.values\"))\n",
        "  else:\n",
        "    py.add_line(py.assign(\"y\", f\"df.{target}.values\"))\n",
        "\n",
        "  py.add_line(py.assign(\"model\",py.call(\"Sequential\")))\n",
        "  py.add_line(py.call(\"model.add\", py.call(\"Dense\",50,{\"input_dim\":\"x.shape[1]\", \"activation\":py.str('relu')})))\n",
        "  py.add_line(py.call(\"model.add\", py.call(\"Dense\",25,{\"activation\":py.str('relu')})))\n",
        "  if metadata[META_TYPE] == META_TYPE_REGRESSION:\n",
        "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"1\")))\n",
        "  elif metadata[META_TYPE] == META_TYPE_BINARY_CLASSIFICATION:\n",
        "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"1\",{\"activation\":py.str('sigmoid')})))\n",
        "  else:  \n",
        "    py.add_line(py.call(\"model.add\", py.call(\"Dense\",\"y.shape[1]\",{\"activation\":py.str('softmax')})))\n",
        "  py.add_line(py.call(\"model.compile\", {\"loss\":py.str(loss), \"optimizer\":py.str('adam')}))\n",
        "  py.add_line(py.call(\"model.fit\", \"x\", \"y\", {'verbose':'2','epochs':100}))\n",
        "  print(py.generate())\n",
        "\n",
        "generate_keras(metadata)\n",
        "\n",
        "#py = PythonFile()\n",
        "#py.call(\"test\",\"a\",{'b':'bval','c':'cval'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3p2kabMhX1G",
        "outputId": "44721abf-2c6f-43a4-f610-d537ccaa2aed"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import pandas as pd\n",
            "import io\n",
            "import requests\n",
            "import numpy as np\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Dense, Activation\n",
            "from tensorflow.keras.callbacks import EarlyStopping\n",
            "from scipy.stats import zscore\n",
            "from sklearn.preprocessing import MinMaxScaler\n",
            "x_fields = []\n",
            "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/crx.csv\", na_values=['NA', '?'])\n",
            "df[\"a1\"] = df[\"a1\"].fillna(df[\"a1\"].mode()[0])\n",
            "dummies = pd.get_dummies(df[\"a1\"], prefix=\"a1\", drop_first=True)\n",
            "df = pd.concat([df,dummies],axis=1)\n",
            "x_fields += dummies.columns.tolist()\n",
            "df[\"a2\"] = df[\"a2\"].fillna(df[\"a2\"].median())\n",
            "df[\"a2\"] = zscore(df[\"a2\"])\n",
            "x_fields.append(\"a2\")\n",
            "df[\"s3\"] = MinMaxScaler().fit_transform(df[['s3']])\n",
            "x_fields.append(\"s3\")\n",
            "df[\"a4\"] = df[\"a4\"].fillna(df[\"a4\"].mode()[0])\n",
            "dummies = pd.get_dummies(df[\"a4\"], prefix=\"a4\", drop_first=True)\n",
            "df = pd.concat([df,dummies],axis=1)\n",
            "x_fields += dummies.columns.tolist()\n",
            "df[\"a5\"] = df[\"a5\"].fillna(df[\"a5\"].mode()[0])\n",
            "dummies = pd.get_dummies(df[\"a5\"], prefix=\"a5\", drop_first=True)\n",
            "df = pd.concat([df,dummies],axis=1)\n",
            "x_fields += dummies.columns.tolist()\n",
            "df[\"a6\"] = df[\"a6\"].fillna(df[\"a6\"].mode()[0])\n",
            "dummies = pd.get_dummies(df[\"a6\"], prefix=\"a6\", drop_first=True)\n",
            "df = pd.concat([df,dummies],axis=1)\n",
            "x_fields += dummies.columns.tolist()\n",
            "df[\"a7\"] = df[\"a7\"].fillna(df[\"a7\"].mode()[0])\n",
            "dummies = pd.get_dummies(df[\"a7\"], prefix=\"a7\", drop_first=True)\n",
            "df = pd.concat([df,dummies],axis=1)\n",
            "x_fields += dummies.columns.tolist()\n",
            "df[\"a8\"] = MinMaxScaler().fit_transform(df[['a8']])\n",
            "x_fields.append(\"a8\")\n",
            "dummies = pd.get_dummies(df[\"a9\"], prefix=\"a9\", drop_first=True)\n",
            "df = pd.concat([df,dummies],axis=1)\n",
            "x_fields += dummies.columns.tolist()\n",
            "dummies = pd.get_dummies(df[\"a10\"], prefix=\"a10\", drop_first=True)\n",
            "df = pd.concat([df,dummies],axis=1)\n",
            "x_fields += dummies.columns.tolist()\n",
            "df[\"a11\"] = MinMaxScaler().fit_transform(df[['a11']])\n",
            "x_fields.append(\"a11\")\n",
            "dummies = pd.get_dummies(df[\"a12\"], prefix=\"a12\", drop_first=True)\n",
            "df = pd.concat([df,dummies],axis=1)\n",
            "x_fields += dummies.columns.tolist()\n",
            "dummies = pd.get_dummies(df[\"a13\"], prefix=\"a13\", drop_first=True)\n",
            "df = pd.concat([df,dummies],axis=1)\n",
            "x_fields += dummies.columns.tolist()\n",
            "df[\"a14\"] = df[\"a14\"].fillna(df[\"a14\"].median())\n",
            "df[\"a14\"] = zscore(df[\"a14\"])\n",
            "x_fields.append(\"a14\")\n",
            "df[\"a15\"] = MinMaxScaler().fit_transform(df[['a15']])\n",
            "x_fields.append(\"a15\")\n",
            "x = df[x_fields].values\n",
            "df[\"a16\"] = (df[\"a16\"]==\"+\").astype(int)\n",
            "y = df.a16.values\n",
            "model = Sequential()\n",
            "model.add(Dense(50, input_dim=x.shape[1], activation=\"relu\"))\n",
            "model.add(Dense(25, activation=\"relu\"))\n",
            "model.add(Dense(1, activation=\"sigmoid\"))\n",
            "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
            "model.fit(x, y, verbose=2, epochs=100)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from scipy.stats import zscore\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "x_fields = []\n",
        "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/crx.csv\", na_values=['NA', '?'])\n",
        "df[\"a1\"] = df[\"a1\"].fillna(df[\"a1\"].mode()[0])\n",
        "dummies = pd.get_dummies(df[\"a1\"], prefix=\"a1\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a2\"] = df[\"a2\"].fillna(df[\"a2\"].median())\n",
        "df[\"a2\"] = zscore(df[\"a2\"])\n",
        "x_fields.append(\"a2\")\n",
        "df[\"s3\"] = MinMaxScaler().fit_transform(df[['s3']])\n",
        "x_fields.append(\"s3\")\n",
        "df[\"a4\"] = df[\"a4\"].fillna(df[\"a4\"].mode()[0])\n",
        "dummies = pd.get_dummies(df[\"a4\"], prefix=\"a4\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a5\"] = df[\"a5\"].fillna(df[\"a5\"].mode()[0])\n",
        "dummies = pd.get_dummies(df[\"a5\"], prefix=\"a5\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a6\"] = df[\"a6\"].fillna(df[\"a6\"].mode()[0])\n",
        "dummies = pd.get_dummies(df[\"a6\"], prefix=\"a6\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a7\"] = df[\"a7\"].fillna(df[\"a7\"].mode()[0])\n",
        "dummies = pd.get_dummies(df[\"a7\"], prefix=\"a7\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a8\"] = MinMaxScaler().fit_transform(df[['a8']])\n",
        "x_fields.append(\"a8\")\n",
        "dummies = pd.get_dummies(df[\"a9\"], prefix=\"a9\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "dummies = pd.get_dummies(df[\"a10\"], prefix=\"a10\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a11\"] = MinMaxScaler().fit_transform(df[['a11']])\n",
        "x_fields.append(\"a11\")\n",
        "dummies = pd.get_dummies(df[\"a12\"], prefix=\"a12\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "dummies = pd.get_dummies(df[\"a13\"], prefix=\"a13\", drop_first=True)\n",
        "df = pd.concat([df,dummies],axis=1)\n",
        "x_fields += dummies.columns.tolist()\n",
        "df[\"a14\"] = df[\"a14\"].fillna(df[\"a14\"].median())\n",
        "df[\"a14\"] = zscore(df[\"a14\"])\n",
        "x_fields.append(\"a14\")\n",
        "df[\"a15\"] = MinMaxScaler().fit_transform(df[['a15']])\n",
        "x_fields.append(\"a15\")\n",
        "x = df[x_fields].values\n",
        "df[\"a16\"] = (df[\"a16\"]==\"+\").astype(int)\n",
        "y = df.a16.values\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=x.shape[1], activation=\"relu\"))\n",
        "model.add(Dense(25, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "model.fit(x, y, verbose=2, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exzeWtiInF5H",
        "outputId": "36af5aac-1963-4d74-e244-c337b109717f"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "22/22 - 1s - loss: 0.6542 - 551ms/epoch - 25ms/step\n",
            "Epoch 2/100\n",
            "22/22 - 0s - loss: 0.5799 - 44ms/epoch - 2ms/step\n",
            "Epoch 3/100\n",
            "22/22 - 0s - loss: 0.4934 - 48ms/epoch - 2ms/step\n",
            "Epoch 4/100\n",
            "22/22 - 0s - loss: 0.4169 - 43ms/epoch - 2ms/step\n",
            "Epoch 5/100\n",
            "22/22 - 0s - loss: 0.3630 - 47ms/epoch - 2ms/step\n",
            "Epoch 6/100\n",
            "22/22 - 0s - loss: 0.3313 - 41ms/epoch - 2ms/step\n",
            "Epoch 7/100\n",
            "22/22 - 0s - loss: 0.3120 - 57ms/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "22/22 - 0s - loss: 0.3036 - 43ms/epoch - 2ms/step\n",
            "Epoch 9/100\n",
            "22/22 - 0s - loss: 0.2950 - 52ms/epoch - 2ms/step\n",
            "Epoch 10/100\n",
            "22/22 - 0s - loss: 0.2878 - 63ms/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "22/22 - 0s - loss: 0.2839 - 48ms/epoch - 2ms/step\n",
            "Epoch 12/100\n",
            "22/22 - 0s - loss: 0.2763 - 59ms/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "22/22 - 0s - loss: 0.2731 - 59ms/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "22/22 - 0s - loss: 0.2663 - 50ms/epoch - 2ms/step\n",
            "Epoch 15/100\n",
            "22/22 - 0s - loss: 0.2632 - 50ms/epoch - 2ms/step\n",
            "Epoch 16/100\n",
            "22/22 - 0s - loss: 0.2593 - 44ms/epoch - 2ms/step\n",
            "Epoch 17/100\n",
            "22/22 - 0s - loss: 0.2539 - 50ms/epoch - 2ms/step\n",
            "Epoch 18/100\n",
            "22/22 - 0s - loss: 0.2504 - 54ms/epoch - 2ms/step\n",
            "Epoch 19/100\n",
            "22/22 - 0s - loss: 0.2494 - 46ms/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "22/22 - 0s - loss: 0.2438 - 45ms/epoch - 2ms/step\n",
            "Epoch 21/100\n",
            "22/22 - 0s - loss: 0.2400 - 39ms/epoch - 2ms/step\n",
            "Epoch 22/100\n",
            "22/22 - 0s - loss: 0.2358 - 42ms/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "22/22 - 0s - loss: 0.2311 - 49ms/epoch - 2ms/step\n",
            "Epoch 24/100\n",
            "22/22 - 0s - loss: 0.2299 - 44ms/epoch - 2ms/step\n",
            "Epoch 25/100\n",
            "22/22 - 0s - loss: 0.2246 - 56ms/epoch - 3ms/step\n",
            "Epoch 26/100\n",
            "22/22 - 0s - loss: 0.2213 - 50ms/epoch - 2ms/step\n",
            "Epoch 27/100\n",
            "22/22 - 0s - loss: 0.2175 - 42ms/epoch - 2ms/step\n",
            "Epoch 28/100\n",
            "22/22 - 0s - loss: 0.2169 - 48ms/epoch - 2ms/step\n",
            "Epoch 29/100\n",
            "22/22 - 0s - loss: 0.2114 - 59ms/epoch - 3ms/step\n",
            "Epoch 30/100\n",
            "22/22 - 0s - loss: 0.2117 - 44ms/epoch - 2ms/step\n",
            "Epoch 31/100\n",
            "22/22 - 0s - loss: 0.2046 - 41ms/epoch - 2ms/step\n",
            "Epoch 32/100\n",
            "22/22 - 0s - loss: 0.2031 - 55ms/epoch - 2ms/step\n",
            "Epoch 33/100\n",
            "22/22 - 0s - loss: 0.2023 - 41ms/epoch - 2ms/step\n",
            "Epoch 34/100\n",
            "22/22 - 0s - loss: 0.1943 - 41ms/epoch - 2ms/step\n",
            "Epoch 35/100\n",
            "22/22 - 0s - loss: 0.1913 - 43ms/epoch - 2ms/step\n",
            "Epoch 36/100\n",
            "22/22 - 0s - loss: 0.1923 - 42ms/epoch - 2ms/step\n",
            "Epoch 37/100\n",
            "22/22 - 0s - loss: 0.1859 - 52ms/epoch - 2ms/step\n",
            "Epoch 38/100\n",
            "22/22 - 0s - loss: 0.1828 - 47ms/epoch - 2ms/step\n",
            "Epoch 39/100\n",
            "22/22 - 0s - loss: 0.1793 - 43ms/epoch - 2ms/step\n",
            "Epoch 40/100\n",
            "22/22 - 0s - loss: 0.1761 - 41ms/epoch - 2ms/step\n",
            "Epoch 41/100\n",
            "22/22 - 0s - loss: 0.1738 - 49ms/epoch - 2ms/step\n",
            "Epoch 42/100\n",
            "22/22 - 0s - loss: 0.1722 - 44ms/epoch - 2ms/step\n",
            "Epoch 43/100\n",
            "22/22 - 0s - loss: 0.1671 - 49ms/epoch - 2ms/step\n",
            "Epoch 44/100\n",
            "22/22 - 0s - loss: 0.1654 - 53ms/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "22/22 - 0s - loss: 0.1663 - 46ms/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "22/22 - 0s - loss: 0.1651 - 44ms/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "22/22 - 0s - loss: 0.1572 - 43ms/epoch - 2ms/step\n",
            "Epoch 48/100\n",
            "22/22 - 0s - loss: 0.1561 - 56ms/epoch - 3ms/step\n",
            "Epoch 49/100\n",
            "22/22 - 0s - loss: 0.1522 - 41ms/epoch - 2ms/step\n",
            "Epoch 50/100\n",
            "22/22 - 0s - loss: 0.1507 - 43ms/epoch - 2ms/step\n",
            "Epoch 51/100\n",
            "22/22 - 0s - loss: 0.1502 - 55ms/epoch - 3ms/step\n",
            "Epoch 52/100\n",
            "22/22 - 0s - loss: 0.1468 - 43ms/epoch - 2ms/step\n",
            "Epoch 53/100\n",
            "22/22 - 0s - loss: 0.1440 - 43ms/epoch - 2ms/step\n",
            "Epoch 54/100\n",
            "22/22 - 0s - loss: 0.1400 - 55ms/epoch - 3ms/step\n",
            "Epoch 55/100\n",
            "22/22 - 0s - loss: 0.1384 - 50ms/epoch - 2ms/step\n",
            "Epoch 56/100\n",
            "22/22 - 0s - loss: 0.1420 - 43ms/epoch - 2ms/step\n",
            "Epoch 57/100\n",
            "22/22 - 0s - loss: 0.1330 - 61ms/epoch - 3ms/step\n",
            "Epoch 58/100\n",
            "22/22 - 0s - loss: 0.1336 - 49ms/epoch - 2ms/step\n",
            "Epoch 59/100\n",
            "22/22 - 0s - loss: 0.1297 - 46ms/epoch - 2ms/step\n",
            "Epoch 60/100\n",
            "22/22 - 0s - loss: 0.1338 - 50ms/epoch - 2ms/step\n",
            "Epoch 61/100\n",
            "22/22 - 0s - loss: 0.1280 - 39ms/epoch - 2ms/step\n",
            "Epoch 62/100\n",
            "22/22 - 0s - loss: 0.1247 - 46ms/epoch - 2ms/step\n",
            "Epoch 63/100\n",
            "22/22 - 0s - loss: 0.1214 - 50ms/epoch - 2ms/step\n",
            "Epoch 64/100\n",
            "22/22 - 0s - loss: 0.1224 - 44ms/epoch - 2ms/step\n",
            "Epoch 65/100\n",
            "22/22 - 0s - loss: 0.1179 - 56ms/epoch - 3ms/step\n",
            "Epoch 66/100\n",
            "22/22 - 0s - loss: 0.1173 - 64ms/epoch - 3ms/step\n",
            "Epoch 67/100\n",
            "22/22 - 0s - loss: 0.1146 - 43ms/epoch - 2ms/step\n",
            "Epoch 68/100\n",
            "22/22 - 0s - loss: 0.1136 - 45ms/epoch - 2ms/step\n",
            "Epoch 69/100\n",
            "22/22 - 0s - loss: 0.1123 - 58ms/epoch - 3ms/step\n",
            "Epoch 70/100\n",
            "22/22 - 0s - loss: 0.1101 - 40ms/epoch - 2ms/step\n",
            "Epoch 71/100\n",
            "22/22 - 0s - loss: 0.1095 - 44ms/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "22/22 - 0s - loss: 0.1069 - 44ms/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "22/22 - 0s - loss: 0.1037 - 43ms/epoch - 2ms/step\n",
            "Epoch 74/100\n",
            "22/22 - 0s - loss: 0.1033 - 46ms/epoch - 2ms/step\n",
            "Epoch 75/100\n",
            "22/22 - 0s - loss: 0.1045 - 45ms/epoch - 2ms/step\n",
            "Epoch 76/100\n",
            "22/22 - 0s - loss: 0.1014 - 53ms/epoch - 2ms/step\n",
            "Epoch 77/100\n",
            "22/22 - 0s - loss: 0.0989 - 52ms/epoch - 2ms/step\n",
            "Epoch 78/100\n",
            "22/22 - 0s - loss: 0.0977 - 42ms/epoch - 2ms/step\n",
            "Epoch 79/100\n",
            "22/22 - 0s - loss: 0.0967 - 47ms/epoch - 2ms/step\n",
            "Epoch 80/100\n",
            "22/22 - 0s - loss: 0.0948 - 40ms/epoch - 2ms/step\n",
            "Epoch 81/100\n",
            "22/22 - 0s - loss: 0.0927 - 43ms/epoch - 2ms/step\n",
            "Epoch 82/100\n",
            "22/22 - 0s - loss: 0.0911 - 47ms/epoch - 2ms/step\n",
            "Epoch 83/100\n",
            "22/22 - 0s - loss: 0.0910 - 57ms/epoch - 3ms/step\n",
            "Epoch 84/100\n",
            "22/22 - 0s - loss: 0.0918 - 53ms/epoch - 2ms/step\n",
            "Epoch 85/100\n",
            "22/22 - 0s - loss: 0.0882 - 44ms/epoch - 2ms/step\n",
            "Epoch 86/100\n",
            "22/22 - 0s - loss: 0.0887 - 59ms/epoch - 3ms/step\n",
            "Epoch 87/100\n",
            "22/22 - 0s - loss: 0.0861 - 47ms/epoch - 2ms/step\n",
            "Epoch 88/100\n",
            "22/22 - 0s - loss: 0.0843 - 42ms/epoch - 2ms/step\n",
            "Epoch 89/100\n",
            "22/22 - 0s - loss: 0.0851 - 48ms/epoch - 2ms/step\n",
            "Epoch 90/100\n",
            "22/22 - 0s - loss: 0.0846 - 45ms/epoch - 2ms/step\n",
            "Epoch 91/100\n",
            "22/22 - 0s - loss: 0.0806 - 41ms/epoch - 2ms/step\n",
            "Epoch 92/100\n",
            "22/22 - 0s - loss: 0.0783 - 44ms/epoch - 2ms/step\n",
            "Epoch 93/100\n",
            "22/22 - 0s - loss: 0.0799 - 55ms/epoch - 3ms/step\n",
            "Epoch 94/100\n",
            "22/22 - 0s - loss: 0.0753 - 45ms/epoch - 2ms/step\n",
            "Epoch 95/100\n",
            "22/22 - 0s - loss: 0.0772 - 43ms/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "22/22 - 0s - loss: 0.0759 - 47ms/epoch - 2ms/step\n",
            "Epoch 97/100\n",
            "22/22 - 0s - loss: 0.0776 - 44ms/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "22/22 - 0s - loss: 0.0736 - 43ms/epoch - 2ms/step\n",
            "Epoch 99/100\n",
            "22/22 - 0s - loss: 0.0737 - 43ms/epoch - 2ms/step\n",
            "Epoch 100/100\n",
            "22/22 - 0s - loss: 0.0742 - 57ms/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc37f88eca0>"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['a16']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq35lJHtBGjG",
        "outputId": "f88e6b59-190a-4f39-80b1-4426b0bb740a"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "685    0\n",
              "686    0\n",
              "687    0\n",
              "688    0\n",
              "689    0\n",
              "Name: a16, Length: 690, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/crx.csv\", na_values=['NA', '?'])"
      ],
      "metadata": {
        "id": "OzkKfoNMOM7Z"
      },
      "execution_count": 137,
      "outputs": []
    }
  ]
}